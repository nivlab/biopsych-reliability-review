% set document class
\documentclass[a4paper,notitlepage,12pt]{article}

% load packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage[backend=biber,style=nature]{biblatex}

% specify bibliography source
\addbibresource{main.bib}
\renewcommand*{\bibfont}{\small}

% specify link style
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% specify some custom commands
\renewcommand{\baselinestretch}{1.2}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\normalsize}

% set up the author block
\setlength{\affilsep}{2em}   % set the space between author and affiliation
\author[1]{Samuel Zorowitz}
\author[1,2]{Yael Niv}
\affil[1]{Princeton Neuroscience Institute, Princeton University, USA}
\affil[2]{Department of Psychology, Princeton University, USA}

% specify the title
\title{Improving the reliability of cognitive task measures: A narrative review}

% turn date off
\date{}

% Define paragraph formatting
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section{Introduction}

% P1: Using valid and reliable measures of behavior is crucial for advancing biological psychiatry.

Cognitive tasks have already demonstrated, and continue to hold, tremendous promise for biological psychiatry. Here we define cognitive task as any experimental paradigm that measures some behavior in order to make inferences about one or more cognitive processes (e.g. Stroop task, delay discounting task, reversal learning task). Variability in performance on cognitive tasks provide experimenters with crucial insights into how individual differences in cognitive processing and ability relate to behavioral phenomena outside of the laboratory or clinic (e.g. reward sensitivity in a monetary-incentivized task and anhedonia in depression). Elsewhere in psychology, cognitive tasks have been useful in predicting important outcomes such as academic achievement \cite{peng2019meta, spiegel2021relations} and cognitive decline \cite{salthouse2010selective}. Cognitive tasks then are invaluable tools for building and refining our understanding of psychiatric symptoms and syndromes. For a cognitive task to be useful in this regard, however, it must possess sufficient measurement properties. 

% P2: Three key properties of behavioral measures: discriminability, reliability, and validity

The psychometric quality of a cognitive task measure (i.e. any descriptive or model-based indicator of behavioral performance such as proportion correct, average response time, and learning rate) for characterizing individual differences are summarized by three key properties: discriminating power, reliability, and validity \cite{kline2015handbook}. The discriminatory power of a task measure describes its ability to produce variability in participants' performance. This is a necessary property of tasks to be used for studying individual differences; where there is no variation in performance, there are no individual differences to study. The reliability of a task characterizes the degree to which it consistently rank-order participants across measurements. Put another way, a task measure is reliable if, assuming participants have not changed, it produces the same score for each participant over time. Finally, the validity of a task measure concerns whether it actually measures what it purports to measure. The focus of the current review is task reliability, which is a prerequisite for validity. 

% P3: definition and Importance of reliability

What is the formal definition of reliability? In classical test theory, the variance in observed scores on a task measure (X) is the sum of the true score variance (T), reflecting real individual differences in the latent construct of interest, and measurement error (E; i.e. X = T + E). The reliability of a measure is defined as the proportion of variance attributable to the true score variance relative to total variance (i.e. R = T/[T + E]). In other words, reliability quantifies the magnitude of individual differences relative to the noisiness of a task measure; the larger the reliability of a task measure, the more it reflects true individual differences and the less it reflects noise.. It should now be clearer why reliability is a prerequisite for validity; when a task measure is unreliable, it reflects measurement error and not the construct of interest. If this were not reason enough to care about reliability, the observed correlation between two measures (e.g. task measure and self-reported symptom score) is a function of their individual reliabilities \cite{Spearman1904-mo}: 

\begin{equation}
    \rho_{xy} = \rho_{tt} \sqrt{\rho_{xx'} \cdot \rho_{yy'}}
\end{equation}

where $\rho_{xx'}$ and $\rho_{yy'}$ are the reliabilities of two measures, $x$ and $y$; $\rho_{tt}$ is their true latent correlation; and  $\rho_{xy}$ is their observed correlation. As can be observed in the equation above, the reliability of a measure places an upper bound on the maximum observable correlation between itself and a second measure. As reliability decreases, so too does the maximum observable correlation between itself and any other measure (Figure 1). As an important corollary, as reliability decreases, the number of participants required to reliably detect a correlation between two measures increases \cite{Parsons2019-jw}. Thus, poor reliability hampers our ability to investigate associations between cognitive processes, as measured by task performance, and other variables of interest. 

% P4: Reliability is inherently context-dependent: it is a function of the population, task, and testing context

To further complicate matters, the reliability of a task measure is not universal. The reliability of a task reflects interactions between its design, the participants completing it, and the context in which it is administered. Indeed task reliability can vary as a function of experimental design (stimulus sets, number of trials, time limits \cite{paap2016role, cooper2017role}); sample populations (healthy adults, children, psychiatric patients \cite{arnon2020current, cooper2017role}); testing locations (in-clinic, online); and response modality (desktop, smartphone, virtual reality \cite{pronk2022can, bruder2021reliability}); and scoring or estimation method \cite{Rouder2019-am, haines2020learning}. For example, a cognitive task originally designed for use with an adult population may prove too difficult for children; their task performance may drop to chance-level, thereby minimizing between-participant variance and, as a consequence, task reliability. As a second example, participants completing an experiment online from their homes may experience more distraction than if they participated in the lab. This may in turn lead to an increase in measurement error and a concomitant decrease in reliability. Experimenters therefore cannot assume the reliability of a task measure is constant. Ideally then, experimenters should investigate and report the reliability of task measures as part of any analysis or, at the very least, after having made changes to a task or scoring procedure or when administering the task to new sample populations or in new testing contexts.

% P5: Task reliability lower on average

Though verifying the reliability of cognitive task measures is paramount to successful individual differences research, the reliability of task measures are historically seldom reported \cite{Green2016-xw, Parsons2019-jw}. When reported, it has frequently been observed that task measures exhibit lower reliability than what is conventionally considered acceptable for individual differences research (e.g. $\rho$ = 0.7 - 0.8) and lower than what is regularly achieved by self-report measures. Indeed many studies have now found that task measures to exhibit moderate-to-low reliability \cite{Hedge2018-lf, Frey2017-uz, Enkavi2019-oh, Von_Bastian2020-tm, Nitsch2022-pe, verdejo2021unified}. One possible explanation for this finding is the so-called ``reliability paradox'' of cognitive tasks \cite{hedge2018reliability}, which states that the often lackluster reliability of tasks is a result of a mismatch in goals between experimental and individual differences psychological research. In experimental psychology, the goal is often to demonstrate the existence of a behavioral effect. One sure-fire means of increasing the power to detect an effect is to minimize between-participants variance of a task measure. This of course is the exact opposite of what is desirable for individual differences research, where between-participants variance is essential to achieving reliable task measures. Thus, the tendency in biological psychiatry to adopt the most prominent tasks in experimental psychology --- the ones that most reliably demonstrate a behavioral effect --- may actually hamstring our efforts to study individual differences. 

% P6: Purpose of paper: narrative review of task reliability

Regardless we do not believe that task measures are inherently less reliable and that pessimism about task-based individual differences research is warranted. It is possible to (re)design tasks to achieve good reliability, even to the high levels dictated by conventional standards \cite{waltmann2022sufficient, sullivan2022enhancing, kucina2022solution, snijder2022psychometric}. The purpose of the current article is to provide a narrative review of approaches to improve task measure reliability. The article is divided into two main parts. First, we review methods of calculating reliability and discuss some nuances that are specific to cognitive tasks. Then, we introduce a taxonomy of approaches for improving task reliability using concrete examples from the published literature. We hope that this article can serve as a helpful guide for experimenters who wish to design a new task, or improve an existing task, to achieve sufficient reliability for use in individual differences research.

\section{Calculating reliability}

We begin with a brief discussion of calculating reliability for cognitive task measures. There are two measures of reliability of relevant for task measures: internal consistency and test-retest reliability. Internal consistency is the reliability of a measure in a single administration. In turn, test-retest reliability (also known as temporal stability) is reliability of a task measure across two or more administrations. The two types of reliability provide complimentary information: internal consistency quantifies the consistency of individual difference on a task measure in a given testing session, whereas test-retest reliability quantifies the stability of individual differences across time. Even for experimenters primarily interested in cross-sectional experiments, test-retest reliability is a useful index. This is because estimates of internal consistency tend to be inflated due to between-participants variance from construct-irrelevant state-dependent factors (e.g. current mood, fatigue). Given sufficient time between testing sessions, test-retest reliability should be less biased state factors. A second point to consider with respect to these two indices is that, whereas high internal consistency is virtually always desirable for individual difference studies, high test-retest reliability may not be. Test-retest reliability should be interpreted with respect to the construct of interest. For example, low 1-month test-retest reliability may not be problematic for an index of a transient cognitive process (e.g. mood effects on cognition), but may be problematic if the effect is hypothesized to be stable across time (e.g. personality effects on cognition).

The test-retest reliability of a task measure can be calculated in numerous ways \cite{Parsons2019-jw}. Perhaps the simplest approach is to compute the Pearson correlation between participants' scores from two sessions. An alternative approach is to calculate the intraclass correlation coefficient (ICC), which decomposes a task measure into true score variance and error variance. There are many formulas for calculating ICC \cite{Koo2016-fz}, with the critical distinction being reliability is based on the \textit{consistency} or \textit{absolutely agreement} of a task measure across two administration. Consistency-based ICCs are affected only by the relative ordering of participants across time; that is, they are insensitive to systematic changes to the actual values of a task measure across time (e.g. practice effects). In contrast, absolute agreement-based ICCs measure the degree to which scores are identical across time. They are sensitive to changes in the overall mean of a task measure across sessions. The type of ICC to use depends on the experimenter's goals. If an experimenter cares only about the rank-ordering of participants across time, a consistency-based ICC (or Pearson correlation) is preferable. Conversely, if systematic differences in a task measure across task administrations is a concern, then an absolute agreement-based ICC should be preferred. 

Calculating the internal consistency is more complicated for task measures. The most common measure of internal consistency, Cronbach's $\alpha$, requires certain assumptions that are unrealistic for task data (e.g. equivalence of trials, uncorrelated measurement error). As such, internal consistency for task measures is instead usually calculated via split-$K$ reliability instead, where $K$ indicates the number of splits. Split-half reliability ($K=2$) is most frequently chosen. A critical challenge in calculating split-half reliability is in deciding how to partition the data, as estimates of reliability may be biased when partitions are not equally good measures of a behavior \cite{Green2016-xw, pronk2022can}. For example, first-second splitting (i.e. partitioning the data by the first and second halves of an experiment) may be confounded by practice, fatigue, or other linear time effects. Under such conditions, first-second partitioning will spuriously decrease the similarity of data across partitions, thereby increasing estimates of measurement noise and underestimating reliability. In contrast, odd-even splitting (i.e. partitioning the data by odd and even trials) may be confounded when behavior across trials is non-independent (i.e. correlated errors). Under such conditions, odd-even splitting will artificially inflate the similarity of data across partitions, thereby decreasing estimates of measurement noise and overestimating reliability. Therefore, where possible, a permutation-based approach to calculating split-half reliability has been recommended \cite{pronk2022can, Parsons2019-jw}. Briefly, the idea is to compute the average reliability across many thousands of random partitions of the data into halves. For task measures derived from cognitive models, it may be unreasonably computationally intensive to employ such an approach. In which case, it may instead be preferable to design tasks with at least two independent blocks where the model is fit to each block independently and reliability is calculated using the model parameters estimated from each. 

A final point is that, when calculating reliability for task measures, traditional sum or mean score estimates of performance (e.g. proportion correct responses, mean response time) may substantially underestimate reliability \cite{Rouder2019-am, haines2020learning}. These summary scores of task performance neglect trial-by-trial variability in performance, which can result in noisier estimates of task performance and consequently diminished estimates of reliability. Instead, it is often preferable to use trial-level hierarchical (mixed effects) models that can simultaneously estimate between- and within-participant (i.e. trial-level) variability in behavior. By directly modeling trial-by-trial variability, estimates of participants' performance are effectively denoised. Furthermore, the use of hierarchical models allows for partial pooling of trial data across participants, which further increases the precision of estimates of participants' performance. Together these properties can substantially boost estimates of reliability \cite{Rouder2019-am, haines2020learning}. In practice, multiple studies have demonstrated how the use of hierarchical trial-level models directly improves estimates of task measure reliability \cite{snijder2022psychometric, sullivan2022enhancing, brown2020improving, waltmann2022sufficient}. For a detailed discussion of hierarchical models in the context of task reliability, see Haines (this issue).

\section{Improving task reliability}

As defined above, the reliability of a task measure is proportion of variance attributable to between-participant differences relative to measurement error. Thus, there are two major strategies for improving the reliability of a measure: by increasing between-participant variability or by decreasing measurement error. In what follows, we discuss approaches for accomplishing both strategies in turn. Where appropriate, we highlight studies from the published literature that are exemplary for imporiving the reliability of a task measure by implementing a particular strategy. 

\subsection{Increasing between-participant variance}

\subsubsection{Ceiling \& floor effects}

By definition, the reliability of a task measure is zero when there is no variability across participants. Thus, range restriction of task measures via ceiling or floor effects is a serious obstacle to reliability. Siegelman and colleagues \cite{siegelman2017measuring} noted the consequences of floor effects on reliability in the context of statistical learning tasks. In statistical learning tasks, participants must learn to identify subtle patterns in the transition probabilities underlying a continuous sequence of stimuli. In reanalyzing archival datasets, Siegelman and colleagues found a majority of participants were at chance-level performance in discriminating between legitimate and foil sequence patterns; consequently, the reliability of conventional proportion correct measures suffered. In response, the authors designed a new version of a statistical learning task involving stimulus sequences that ranged more widely in their difficulty to learn. Only a minority of participants were at chance-level performance on this new task and, as such, the reliability of proportion correct scores improved. Similarly, in developing an abbreviated working memory task, Oswald and colleagues \cite{oswald2015development} found that they could remove the easiest trials --- those with ceiling level performance --- with virtually no change to task reliability. This is because working memory trials with performance at ceiling are incapable of differentiating ability across participants and therefore cannot contribute significantly to the reliability of the task.

Experimenters who are administering a task to a new population should be wary of range restriction effects. Cognitive tasks calibrated for one group of participants may be too easy or difficult for a different group. For example, Arnon and colleagues \cite{arnon2020current} found that statistical learning tasks developed for adults were too difficult for young children and therefore yielded unreliable discrimination scores in that population. Similarly, Kyllonen and colleagues \cite{kyllonen2019general} were motivated to develop a battery of fluid reasoning measures for highly educated adults due to observed ceiling effects in performance when using preexisting fluid reasoning tasks in this population. In sum, experimenters should be cautious when administering preexisting tasks to new populations; the tasks calibrated for one group may not be adequately sensitive for others.  

\subsubsection{Repeatability \& practice effects}

A related issue for task reliability is practice effects, or when participants' performance on a task improves with repeated administrations. Practice effects are relatively common for cognitive tasks \cite{hausknecht2007retesting, scharfen2018retest}. They might occur due to the attenuation of task-irrelevant nuisance factors (e.g. performance anxiety) and/or the learning of task-specific knowledge or strategies. Practice effects are not inherently an issue for reliability --- especially if an experimenter is only interested in the consistency, but not the absolute agreement, of participants' performance over time --- but they can become a pernicious issue if they are severe enough to induce ceiling effects. For example, Paredes and colleagues \cite{paredes2021psychometric} observed large practice effects on the Pavlovian go/no-go task for short retest intervals (3 days, 14 days), which resulted in poor estimates of test-retest reliability. In developmental and lifespan studies, practice effects are potentially complicated by their interaction with age \cite{anokhin2022age, salthouse2010influence}; that is, practice-induced ceiling effects may present in some age groups but not others. 

One strategy for minimizing practice effects is simply to increase the time interval between task administrations. The more time that elapses between sessions, the greater the probability that participants have forgotten task-specific knowledge or strategies \cite{hausknecht2007retesting, scharfen2018retest}. Of course, this solution may not always be possible or desirable, especially if an experimenter is only able to or specifically interested in studying a behavior over a short time period. 

A second strategy is to design tasks so as to prevent or discourage the formation of task-specific strategies. For example, McLean and colleagues \cite{mclean2018towards} investigated the repeatability of the beads task. In the beads task, participants are presented with two jars containing beads of two colors in equal but opposite ratios. In one trial of the task, a predetermined sequence of beads is drawn from one jar. After participants must decide which jar beads were being drawn from or request to see more beads. In a typical version of the task, the same sequence of bead draws is used across all trials. McLean and colleagues found that participants were aware that the sequence repeated across trials and, as a consequence, became more erratic in their decision to witness more bead draws with additional trials. In response, the authors developed a new version of the beads task that included distractor sequences of bead draws. This new design prevent participants from becoming aware of the target sequence, which in turn motivated participants to respond more consistently thereby improving the reliability of participants' information seeking scores. 

\subsubsection{Enhancing experimental effects}

The preceding sections described potential threats to between-participants variability, but did not address how to actually increase it. A primary strategy for increasing between-participants variability (and therefore reliability) is to enhance the experimental effect. Amplifying the magnitude of an experimental effect (e.g. making a task more challenging, increasing the potency of a stress or affective induction) typically increases the range of participants' responses to it. Two recent studies highlight this approach in the context of cognitive control tasks.  

Kucina and colleagues \cite{kucina2022solution} investigated the reliability of cognitive conflict effects in new versions of several standard cognitive control tasks (e.g. Stroop, Flanker, Simon) that amplified cognitive interference via two task design features. First, they combined multiple sources of cognitive interference in the same task (e.g. combining the Stroop and Simon effects to create a ``Stroopon'' task). Second, for a subset of trials, they required participants to make multiple responses based on both relevant and irrelevant stimuli attributes. Compared to the standard version of these tasks, these experimental manipulations had the effect of increasing task demands, which resulted in greater between-participants variance and reliability as a consequence. Similarly, Snijder and colleagues \cite{snijder2022psychometric} studied the reliability of cognitive conflict effects in multiple redesigned cognitive control tasks (e.g. Stroop, AX-CPT, Cued Task Switching, Sternberg) that required of participants either more proactive control (i.e. maintained vigilance) or reactive control. The authors found that the proactive control versions of the task were more demanding, which similarly led to larger cognitive interference effects and ultimately task reliability. 

For decision-making tasks, or other tasks requiring participants to choose between two or more discrete options, a related strategy is to calibrate the difficulty of trials to the average ability of the population of interest. For example, consider a task trial with two response options. Given the Bernoulli distribution, the variance in responses on this trial is maximized when the probability of choosing either response is equal (i.e. 50\%/50\%). Thus, aggregating across many trials, between-participants variance is maximized --- and task reliability is improved --- when the difficulty of all items are around the average ability of the sample \cite{gulliksen1945relation, ferrando2007external} (or somewhere above this point when guessing the correct response is an issue \cite{lord1952relation, feldt1993relationship}). Of course, this design principle is only helpful to the degree that an experimenter  precisely knows the average ability level of their participants. If this is unknown or poorly characterized, then it is instead desirable to design a task with trials spanning a uniform range of difficulty. 

\subsubsection{Sample population}

A final strategy for increasing between-participant variance is to simply recruit more diverse samples. While convenient, undergraduate students from a single university are likely to be relatively homogeneous in their cognitive profiles. It may be worthwhile instead to recruit participants from the community or from an online labor market (e.g. Amazon Mechanical Turk, Prolific Academic, CloudResearch Panels). With regard to the latter, it is worth mentioning that, because online participants are typically completing experiments from their homes, they may be more likely to be distracted or multi-tasking during an experiment \cite{newman2021data}. Thus, when recruiting online samples, experimenters should take special care to ensure that an increase in between-participants variance is not offset by a concomitant increase in measurement noise.

\subsection{Decreasing measurement noise}

\subsubsection{Increasing trial numbers}

Perhaps the most straightforward approach to decreasing measurement error, and thereby increasing reliability, is to increase the number of task trials. In generalizability theory, the relationship between reliability and the number of trials defined as:

\begin{equation}
    \rho = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E / n}
\end{equation}

where $\sigma^2_T$ is the true between-participants variance, $\sigma^2_E$ is measurement error (i.e. trial-level variance), and $n$ is the number of trials. In practice, this relationship often holds (e.g. \cite{paap2016role, cooper2017role}) but there are sometimes exceptions \cite{price2015empirical, klingelhoefer2022robust}. Increasing the number of task trials only benefits reliability if measurement error is random. If increasing task length results in participant fatigue or boredom, then measurement noise may systematically increase and, as a consequence, reliability will suffer. Furthermore, even if increasing the trial number does not induce fatigue, it may still be impractical for other reasons. Because increasing the trial number yields diminishing marginal improvements for reliability, achieving a desired level of reliability may require prohibitively long experiments, especially when the task is completed as part of a larger battery.

\subsubsection{Improving experiment designs}

Measurement error can be reduced through improving the design of experiments, which can be accomplished in many ways. The reliability of a task measure can be improved by including in an experiment only the most discriminating stimuli. For example, in the context of an emotion recognition task, stimuli of good discriminability with would be those where participants with good emotion recognition ability consistently correctly identify the displayed emotion and were participants with poor ability consistently incorrectly identify the displayed emotion \cite{keutmann2015generating}. In contrast, stimuli with poor discriminability --- those for which performance between high and low ability participants is indistinguishable --- will lead to more measurement noise and decreased reliability. In experiments where stimuli are intended to be unique and distinguishable, it is important to ensure that participants are not confusing the identities of the stimuli. Improving both the linguistic and visual distinctness of stimuli may prevent this sort of confusion and therefore aid reliability \cite{yoo2022importance}.  

Other design features of an experiment may affect reliability. Consider for example the dot-probe task, in which participants must disengage attention from a distracting image on one part of the screen in order to identify and respond to the orientation of a pair of dots elsewhere on the screen. Previous research has found that dot-bottom trials, where a participant disengage from a distracting stimulus located at the top of the screen and then saccade to the bottom of the screen, are more reliable than dot-top trials \cite{price2015empirical, aday2019extended}. An explanation for this finding is that, if participants tend to have a gaze bias towards the top of the screen, a saccade away from the top of the screen might require a stronger level of disengagement. As such, dot-bottom trials are sometime found to be a more robust signal of attentional bias. In summary, the actual mechanics of a task trial may have profound influence on task reliability 

It is worth stressing that clear, concise instructions are essential for task reliability. When participants are unsure of what they are intended to do in an experiment, their behavior is likely to be more erratic and random, thereby diminishing reliability. (Clear instructions may also work to ensure the validity of an experiment by discouraging participants from using cognitive strategies not of interest to the experimenter.) In their 10 simple rules paper for designing cognitive experiments, Barbosa and colleagues \cite{barbosa2022practical} provide practical suggestions for writing task instructions. When adapting a task for a new population, experimenters should ensure that the instructions are still appropriate. Task instructions that are comprehensible to healthy adult participants may not be suitable for other populations like children \cite{hughes2002measuring}. 

Another strategy to reduce measurement error is to incorporate practice trials into an experiment. Practice trials can help to minimize the effects of nuisance factors such as performance anxiety or unfamiliarity with the response modality. Moreover, practice trials give participants participants an additional opportunity to make sure they understand the task instructions. Thus, practice trials can help participants reach a more consistent ``steady state'' of behavior, thereby reducing the noisiness of their responses and increasing reliability \cite{alexander2003effects}. If experimenters are unable to include a standalone practice block in their experimenters due to time constraints, they can implicitly incorporate practice trials by designating and discarding (or modeling separately) the first handful of trials during an experiment \cite{mclean2018towards}. 

Yet another strategy to diminish measurement error is to ``gamify'' an experiment. The gamification of cognitive tasks, --- incorporating (video) game design elements into cognitive tasks --- can increase participant engagement and motivation during an experiment \cite{sailer2017gamification}. Keeping participants motivated and engaged may counter the would-be effects of boredom and fatigue on task reliability. For example, Kucina and colleagues \cite{kucina2022solution} cite task gamification as an important factor that contributed to the improved the reliability of their cognitive control tasks. Similarly, Verdejo and colleagues \cite{verdejo2021unified} partially attribute the improved reliability of their impulsivity task battery to gamified task design. 

\subsubsection{Reducing parameter estimation noise}

When the parameters from cognitive models are used as summaries of participants' task performance, another means to improve measure reliability is to decrease estimation noise. The estimation noise of a parameter given an experiment and model can be quantified through simulation studies \cite{wilson2019ten, palminteri2017importance}. In a simulation study, an experimenter generates artificial data for the experiment using representative model parameters. The experimenter attempts to recover the model parameters by fitting the model to the simulated data. Estimation noise is the inverse of the (relative or absolute) agreement between the true and recovered parameters. Alterations to experimental design can improve parameter recovery and estimation noise, and multiple frameworks have been proposed for testing and improving experimental designs to aid parameter recovery \cite{broomell2014parameter, melinscak2020computational}. Parameter recovery can also be affected by model estimation method \cite{Lerche2017-yg, waltmann2022sufficient}. As mentioned above, the partial pooling properties of hierarchical Bayesian models can be especially beneficial for improving parameter recovery and decreasing estimation noise \cite{katahira2016hierarchical}.

A related approach is to use adaptive experimental designs \cite{myung2013tutorial}. In an adaptive experiment, the design of an experiment is generated on-line in order to present each participant with stimuli or trial types that are matched to their particular response patterns or ability levels. Though undoubtedly a more complex experimental design, adaptive experiments have the advantage of selecting the most informative trials for resolving the ability or preference level of a participant (e.g. as measured by a cognitive model). Furthermore, adaptive designs  may require fewer trials to achieve precision measurement, which can save experimenters (and participants) time and money. Adaptive designs have been successfully translated in cognitive research for example in working memory \cite{gonthier2018measuring} and delay discounting \cite{ahn2020rapid}. For a detailed discussion of adaptive design experiments, see Ahn (this issue). 

Parameter estimation may be further improved by leveraging additional information. For example, latent variables may be more accurately measured through the inclusion of demographic variables or other covariates during estimation \cite{anders2018improved, Curran2016-bt}. The basic idea is that, if there are any associations between model parameters and covariates, they can aid in resolving parameter estimates. An extension of this idea is to utilize joint modeling of dependent variables; that is, to design models where multiple observed trial-level variables are predicted simultaneously. For example, the joint modeling of choice and response time has been found to improve the precision and reliability of estimated parameters in cognitive ability testing \cite{bertling2018using} and reinforcement learning \cite{ballard2019joint, shahar2019improving}. It is also possible to incorporate physiological and/or neural correlates of behavior, such as skin conductance response, fMRI BOLD signal, and EEG. 

\subsection{Difference scores}

In the context of reliability, difference scores deserve special treatment. Difference scores describe a measure where a measure of a participant's performance from one condition is subtracted from their performance in another. An example of a difference score is the classic Stroop interference effect; that is, the difference in average reaction time between congruent and incongruent trials. The value of difference scores is straightforward: difference scores allow experimenters to isolate particular cognitive processes (e.g. processing cognitive conflict) and to control for irrelevant sources of variance (e.g. perceptual processing, motor ability). Given their utility, difference scores are ubiquitous in experimental psychology. 

The challenge for difference scores is that their reliability is a function of the reliability of their components \textit{and} the correlation between the components:

\begin{equation}
    \rho_{dd'} = \frac{\sigma^2_x \rho_{xx'} + \sigma^2_y \rho_{yy'} - 2 \rho_{xy} \sigma_x \sigma_y}{\sigma^2_x + \sigma^2_y - 2 \rho_{xy} \sigma_x \sigma_y}
\end{equation}

where $\sigma^2_x$ and $\sigma^2_y$ are the variances of task measures \textit{x} and \textit{y}; $\rho_{xx'}$ + $\rho_{yy'}$ are the reliabilities of task measures \textit{x} and \textit{y}; and $\rho_{xy}$ is the correlation between task measures \textit{x} and \textit{y} \cite{trafimow2015defense, chiou1996reliability}. When the variances of the two measures are equal, this reduces to: 

\begin{equation}
    \rho_{dd'} = \frac{\rho_{xx'} + \rho_{yy'} - 2 \rho_{xy}}{2 - 2 \rho_{xy}}
\end{equation}

From the above equation, it is readily apparent that the reliability of a difference score measure is diminished to the extent that its components are correlated. In experimental psychology, two measures derived from the same task will often be correlated due to shared domain-general cognitive processes. Thus, it will often (if not always) be the case that difference scores derived from task measures will be less reliable than their components. This presents a challenge for designing reliable difference score measures. 


\subsubsection{Identify alternative measures}

An alternative approach to improving the reliability of difference scores is simply to avoid using them in the first place. That is, experimenters may opt to design tasks or analyses in such a way to avoid the necessity of difference scores in the first place. This recommendation has a long history in experimental psychology. Indeed, because difference scores will virtually always be less reliable, many authors have advocated to simply abandon them \cite{lord1956measurement, cronbach1970we, edwards2001ten}. 

What then are the alternatives to difference scores? Draheim and colleagues \cite{draheim2019reaction} provide an in-depth review of alternatives to difference scores in the context of response time measures, though much of their discussion is applicable to difference scores in general. One possibility is to simply use the component measures (e.g. performance on incongruent trials in a Stroop task alone). Of course, because component scores will be contaminated other sources of variance, such as baseline performance, interpreting component scores should be done with caution. Another approach is to identify alternative measures of task performance. For example, intra-individual response time variability has been identified as a correlate in executive control, with aberrations in psychopathology broadly \cite{kofler2013reaction}, that can be measured reliably \cite{saville2011stability}. 

\subsubsection{Enhance \& purify task measures}

When difference scores are unavoidable, there still steps experimenters can take to improve their reliability. As the equations above suggest, the reliability of difference scores can be improved in three ways: (1) improve the reliability of the component measures, (2) increase the differential variability of the task measures, and (3) minimize the correlation between the task measures. The first strategy has been the focus of the discussion so far. The second approach deserves further comment. As noted elsewhere \cite{trafimow2015defense, chiou1996reliability}, the reliability of a difference score measure increases as ratio of measure variances increases. Figure XX shows that, even when the correlation of two measures is large, it is possible to achieve acceptable reliability insofar as the deviation ratio is high. This speaks to another advantage of increasing the between-participants variability of a measure (insofar that an experimenter increases one, but not the other, measure). This result might help explain the improved reliability of the difference score measures studied by Kucina and colleagues \cite{kucina2022solution}.

The third approach is to purify task measures; that is, to decorrelate the components of a difference score by reducing or eliminating their shared variance. Rey-Mermet and colleagues \cite{rey2019executive} provide an interesting example in the context of executive control. In typical executive control tasks, response times on congruent and incongruent trials are highly correlated, reflecting shared variance from conflict-irrelevant processes including baseline processing speed (e.g. perceptual processing, motor speed) and performance strategies (e.g. individual differences in speed-accuracy preferences; \cite{draheim2019reaction}). Noting this, Rey-Mermet and colleagues designed a number of ``response-deadline'' executive control tasks, where participants had a limited amount of time to respond during a trial. Specifically, the duration of the response deadline was calibrated for each participant individually in order to achieve a fixed accuracy level in blocks of neutral trials, which was then used as the deadline for both congruent and incongruent trials. They reasoned the calibration procedure had several important advantages. First, it controls for individual differences in processing speed that contribute to performance in both congruent and incongruent trials. Second, it also controls for individual differences in strategy, because, regardless of whether a participant was biased towards speed or accuracy, inefficient executive control would result in lower accuracy. They found that, with the calibration procedure, the reliability of an accuracy difference score (incongruent - congruent) was as good or better than what had previously been reported for executive control tasks. Thus, controlling for shared variance across measures --- that is, purifying measures --- can help to improve task reliability.

\section{Discussion}

\printbibliography

\end{document}
